{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from pybedtools import BedTool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the data from the following link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set source data path\n",
    "GENCODEPOLYA_PATH=\"../../data/raw_data/annotations/gencode.vM25.polyAs.gtf\"\n",
    "POLYASITES_PATH=\"../../data/raw_data/annotations/atlas.clusters.2.0.GRCm38.96.bed\"\n",
    "POLYADB_PATH=\"../../data/raw_data/annotations/mouse.PAS.txt\"\n",
    "GENCODE_PATH=\"../../data/raw_data/annotations/gencode.vM25.annotation.bed\"\n",
    "\n",
    "# set interval data path\n",
    "GENCODEPOLYA_PROCESSED_PATH=\"../../data/raw_data/annotations/gencode.vM25.polya.processed.bed\"\n",
    "POLYASITES_PROCESSED_PATH=\"../../data/raw_data/annotations/polyasites.mouse.processed.bed\"\n",
    "POLYADB_TOLIFT_PATH=\"../../data/raw_data/annotations/polyadb.mouse.tolift.bed\"\n",
    "POLYADB_PROCESSED_PATH=\"../../data/raw_data/annotations/polyadb.mouse.processed.bed\"\n",
    "\n",
    "# set output data path\n",
    "GENCODE_PROCESSED_PATH=\"../../data/raw_data/annotations/gencode.vM25.annotation.bed\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process genome annotation\n",
    "gencode_df = pd.read_csv(\n",
    "    GENCODE_PATH, sep=\"\\t\", header=None\n",
    ")\n",
    "gencode_df = gencode_df.rename(\n",
    "    columns={\n",
    "        0: \"chr\",\n",
    "        1: \"start\",\n",
    "        2: \"stop\",\n",
    "        4: \"score\",\n",
    "        5: \"strand\",\n",
    "        6: \"source\",\n",
    "        7: \"type\",\n",
    "        9: \"annotation\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gencode_exon_df = gencode_df[gencode_df.type == \"exon\"]\n",
    "gencode_exon_df[['gene_id', 'transcript_id', 'gene_name', 'exon_number']] = gencode_exon_df['annotation'].str.extract('gene_id \"([^\"]*)\";.*transcript_id \"([^\"]*)\";.*gene_name \"([^\"]*)\";.*exon_number ([0-9]*);')\n",
    "gencode_exon_df['exon_number'] = gencode_exon_df['exon_number'].astype(int)\n",
    "\n",
    "# classify exon\n",
    "gencode_exon_df['exon_type'] = 'Internal exon'\n",
    "first_exons = gencode_exon_df.groupby('transcript_id')['exon_number'].idxmin()\n",
    "last_exons = gencode_exon_df.groupby('transcript_id')['exon_number'].idxmax()\n",
    "gencode_exon_df.loc[first_exons, 'exon_type'] = '5\\' most exon'\n",
    "gencode_exon_df.loc[last_exons, 'exon_type'] = '3\\' most exon'\n",
    "exon_counts = gencode_exon_df.groupby('transcript_id').size()\n",
    "single_exon_transcripts = exon_counts[exon_counts == 1].index\n",
    "gencode_exon_df.loc[gencode_exon_df['transcript_id'].isin(single_exon_transcripts), 'exon_type'] = 'Single exon'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gencode_genebody_df = gencode_df[gencode_df.type == \"gene\"]\n",
    "gencode_genebody_df[['gene_id', 'gene_name',]] = gencode_genebody_df['annotation'].str.extract('gene_id \"([^\"]*)\";.*gene_name \"([^\"]*)\";')\n",
    "gencode_genebody_df = gencode_genebody_df.drop_duplicates(subset=['gene_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process gencode polya annotation\n",
    "gencode_polya = pd.read_csv(\n",
    "    GENCODEPOLYA_PATH, sep=\"\\t\", header=None, skiprows=5\n",
    ")\n",
    "gencode_polya = gencode_polya.rename(\n",
    "    columns={\n",
    "        0: \"chr\",\n",
    "        1: \"source\",\n",
    "        2: \"tag\",\n",
    "        3: \"start\",\n",
    "        4: \"stop\",\n",
    "        5: \"score\",\n",
    "        6: \"strand\",\n",
    "        7: \"phase\",\n",
    "        8: \"annotation\",\n",
    "    }\n",
    ")\n",
    "gencode_polya[\"source\"] = \"Gencode\"\n",
    "gencode_polya[\"score\"] = \"1\"\n",
    "gencode_polya_to_save = gencode_polya[\n",
    "    gencode_polya.tag == \"polyA_site\"\n",
    "][[\"chr\", \"start\", \"stop\", \"source\", \"score\", \"strand\"]]\n",
    "gencode_polya_to_save[\"stop\"] = gencode_polya_to_save.apply(lambda x: x[\"start\"] + 1 if x[\"strand\"] == \"+\" else x[\"stop\"], axis=1)\n",
    "gencode_polya_to_save[\"start\"] = gencode_polya_to_save[\"stop\"] - 1\n",
    "gencode_polya_to_save.to_csv(\n",
    "    GENCODEPOLYA_PROCESSED_PATH, index=False, header=None, sep=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process polyasite annotation\n",
    "polyasite = pd.read_csv(\n",
    "    POLYASITES_PATH, sep=\"\\t\", header=None\n",
    ")\n",
    "polyasite = polyasite.rename(\n",
    "    columns={\n",
    "        0: \"chr\",\n",
    "        1: \"start\",\n",
    "        2: \"stop\",\n",
    "        3: \"id\",\n",
    "        4: \"score\",\n",
    "        5: \"strand\",\n",
    "        9: \"annotation\",\n",
    "    }\n",
    ")\n",
    "polyasite.chr = polyasite.apply(lambda x: \"chr\" + str(x.chr), axis=1)\n",
    "polyasite[\"position\"] = polyasite.id.str.split(\":\", expand=True)[1]\n",
    "polyasite[\"start\"] = polyasite.apply(\n",
    "    lambda x: int(x.position) if x.strand == \"+\" else int(x.position) - 1, axis=1\n",
    ")\n",
    "polyasite[\"stop\"] = polyasite.apply(\n",
    "    lambda x: int(x.position) + 1 if x.strand == \"+\" else int(x.position), axis=1\n",
    ")\n",
    "polyasite[\"source\"] = \"PolyASite2.0\"\n",
    "polyasite[\"score\"] = \"1\"\n",
    "# polyasite_to_save = polyasite[\n",
    "#     polyasite.annotation.isin([\"TE\", \"EX\", \"DS\"])\n",
    "# ]\n",
    "polyasite_to_save = polyasite[[\"chr\", \"start\", \"stop\", \"annotation\", \"score\", \"strand\"]]\n",
    "polyasite_to_save[\"stop\"] = polyasite_to_save.apply(lambda x: x[\"start\"] + 1 if x[\"strand\"] == \"+\" else x[\"stop\"], axis=1)\n",
    "polyasite_to_save[\"start\"] = polyasite_to_save[\"stop\"] - 1\n",
    "polyasite_to_save.to_csv(\n",
    "    POLYASITES_PROCESSED_PATH, index=False, header=None, sep=\"\\t\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process polyadb annotation\n",
    "polyadb = pd.read_csv(POLYADB_PATH, sep=\"\\t\")\n",
    "polyadb[\"start\"] = polyadb.apply(\n",
    "    lambda x: int(x.Position) if x.Strand == \"+\" else int(x.Position) - 1, axis=1\n",
    ")\n",
    "polyadb[\"stop\"] = polyadb.apply(\n",
    "    lambda x: int(x.Position) + 1 if x.Strand == \"+\" else int(x.Position), axis=1\n",
    ")\n",
    "polyadb[\"Gene Symbol\"] = polyadb[\"Gene Symbol\"].fillna(\"unknown\")\n",
    "polyadb[\"annotation\"] = polyadb[\"Intron/exon location\"].str.cat(polyadb[[\"Gene Symbol\"]], sep=\":\")\n",
    "polyadb[\"annotation\"] = polyadb[\"annotation\"].str.replace(\" \", \"_\")\n",
    "polyadb[\"score\"] = \"1\"\n",
    "polyadb_to_save = polyadb[\n",
    "    [\"Chromosome\", \"start\", \"stop\", \"annotation\", \"score\", \"Strand\"]\n",
    "]\n",
    "polyadb_to_save[\"stop\"] = polyadb_to_save.apply(lambda x: x[\"start\"] + 1 if x[\"Strand\"] == \"+\" else x[\"stop\"], axis=1)\n",
    "polyadb_to_save[\"start\"] = polyadb_to_save[\"stop\"] - 1\n",
    "polyadb_to_save.to_csv(\n",
    "    POLYADB_TOLIFT_PATH, index=False, header=None, sep=\"\\t\"\n",
    ")\n",
    "# liftOVer from hg19 to hg38\n",
    "os.system(\n",
    "    f\"liftOver {POLYADB_TOLIFT_PATH} \\\n",
    "    ../../data/raw_data/annotations/mm9ToMm10.over.chain \\\n",
    "    {POLYADB_PROCESSED_PATH} \\\n",
    "    ../../data/raw_data/annotations/umap.bed\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "gencode_exon_bed = BedTool.from_dataframe(gencode_exon_df)\n",
    "gencode_genebody_bed = BedTool.from_dataframe(gencode_genebody_df)\n",
    "polyasites_bed = BedTool.from_dataframe(polyasite_to_save)\n",
    "polyadb_bed = BedTool(POLYADB_PROCESSED_PATH)\n",
    "gencode_polya_bed = BedTool.from_dataframe(gencode_polya_to_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义函数用于合并数据集\n",
    "def merge_bed(bed):\n",
    "    bed_pos = bed.filter(lambda b: b.strand == '+').sort().merge(d=10, s=True, c='4,5,6', o='last')\n",
    "    bed_neg = bed.filter(lambda b: b.strand == '-').sort().merge(d=10, s=True, c='4,5,6', o='first')\n",
    "    bed_merged = bed_pos.cat(bed_neg, postmerge=False).sort().to_dataframe()\n",
    "    bed_merged['start'] = bed_merged.apply(lambda x: x['end']-1 if x['strand'] == '+' else x['start'], axis=1)\n",
    "    bed_merged['end'] = bed_merged['start'] + 1\n",
    "    return BedTool.from_dataframe(bed_merged)\n",
    "\n",
    "# 定义一个辅助函数用于获取重叠的PASs\n",
    "# def get_overlapping_pases(bed1, bed2):\n",
    "#     return bed1.window(bed1, w=20, sm=True, u=True)\n",
    "\n",
    "# 合并并去重各个数据集\n",
    "merged_polyasites = merge_bed(polyasites_bed)\n",
    "polyadb_bed_merged = merge_bed(polyadb_bed)\n",
    "gencode_polya_bed_merged = merge_bed(gencode_polya_bed)\n",
    "\n",
    "print(f\"merged_polyasites: {len(merged_polyasites)}\")\n",
    "\n",
    "# 获取PolyA_DB v3的PASs与当前PAS集合中在±10nt范围内有重叠的PAS\n",
    "# overlapping_polyadb = get_overlapping_pases(polyadb_bed_merged, merged_polyasites)\n",
    "overlapping_polyadb = polyadb_bed_merged.window(merged_polyasites, w=20, sm=True, u=True)\n",
    "print(f\"overlapping_polyadb: {len(overlapping_polyadb)}\")\n",
    "polyadb_subtracted = polyadb_bed_merged.subtract(overlapping_polyadb, s=True, A=True)\n",
    "print(f\"polyadb_subtracted: {len(polyadb_subtracted)}\")\n",
    "merged_polyasites_addpd = merged_polyasites.cat(polyadb_subtracted, postmerge=False).sort()\n",
    "\n",
    "# 同样的方法处理Gencode M25的PASs\n",
    "# overlapping_gencode = get_overlapping_pases(gencode_polya_bed_merged, merged_polyasites_addpd)\n",
    "overlapping_gencode = gencode_polya_bed_merged.window(merged_polyasites_addpd, w=20, sm=True, u=True)\n",
    "print(f\"overlapping_gencode: {len(overlapping_gencode)}\")\n",
    "gencode_polya_subtracted = gencode_polya_bed_merged.subtract(overlapping_gencode, s=True, A=True)\n",
    "print(f\"gencode_polya_subtracted: {len(gencode_polya_subtracted)}\")\n",
    "merged_polyasites_addall = merged_polyasites_addpd.cat(gencode_polya_subtracted, postmerge=False).sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib_venn import venn3\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pybedtools import BedTool\n",
    "\n",
    "# polyasites_bed_merged = merge_bed(polyasites_bed)\n",
    "# polyadb_bed_merged = merge_bed(polyadb_bed)\n",
    "# gencode_polya_bed_merged = merge_bed(gencode_polya_bed)\n",
    "\n",
    "# 计算各个集合的大小\n",
    "polyasites_size = len(merged_polyasites_addall.window(merged_polyasites, w=20, sm=True, u=True))\n",
    "polyadb_size = len(merged_polyasites_addall.window(polyadb_bed_merged, w=20, sm=True, u=True))\n",
    "gencode_size = len(merged_polyasites_addall.window(gencode_polya_bed_merged, w=20, sm=True, u=True))\n",
    "\n",
    "# 计算交集的大小\n",
    "polyasites_polyadb_overlap = len(merged_polyasites_addall.window(merged_polyasites, w=20, sm=True, u=True).window(polyadb_bed_merged, w=20, sm=True, u=True))\n",
    "polyasites_gencode_overlap = len(merged_polyasites_addall.window(merged_polyasites, w=20, sm=True, u=True).window(gencode_polya_bed_merged, w=20, sm=True, u=True))\n",
    "polyadb_gencode_overlap = len(merged_polyasites_addall.window(gencode_polya_bed_merged, w=20, sm=True, u=True).window(polyadb_bed_merged, w=20, sm=True, u=True))\n",
    "\n",
    "# # 对window的结果进行处理，只保留前六列\n",
    "# def process_window_result(result):\n",
    "#     processed_result = []\n",
    "#     for line in str(result).split('\\n'):\n",
    "#         columns = line.split('\\t')\n",
    "#         if len(columns) >= 6:\n",
    "#             processed_result.append('\\t'.join(columns[:6]))\n",
    "#     return BedTool('\\n'.join(processed_result), from_string=True)\n",
    "\n",
    "# 计算三个数据集的交集的大小\n",
    "polyasites_polyadb_gencode_overlap = len(merged_polyasites_addall.window(merged_polyasites, w=20, sm=True, u=True).window(polyadb_bed_merged, w=20, sm=True, u=True).window(gencode_polya_bed_merged, w=20, sm=True, u=True))\n",
    "# 计算只在两个数据集中的元素的数量\n",
    "polyasites_polyadb_only = polyasites_polyadb_overlap - polyasites_polyadb_gencode_overlap\n",
    "polyasites_gencode_only = polyasites_gencode_overlap - polyasites_polyadb_gencode_overlap\n",
    "polyadb_gencode_only = polyadb_gencode_overlap - polyasites_polyadb_gencode_overlap\n",
    "\n",
    "# 计算只在一个数据集中的元素的数量\n",
    "polyasites_only = polyasites_size - polyasites_polyadb_overlap - polyasites_gencode_overlap + polyasites_polyadb_gencode_overlap\n",
    "polyadb_only = polyadb_size - polyasites_polyadb_overlap - polyadb_gencode_overlap + polyasites_polyadb_gencode_overlap\n",
    "gencode_only = gencode_size - polyasites_gencode_overlap - polyadb_gencode_overlap + polyasites_polyadb_gencode_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "subsets=(polyasites_only, polyadb_only, polyasites_polyadb_only, gencode_only, polyasites_gencode_only, polyadb_gencode_only, polyasites_polyadb_gencode_overlap)\n",
    "with open(\"../../data/raw_data/annotations/mm10_venn.pkl\", \"wb\") as f:\n",
    "    pickle.dump(subsets, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成韦恩图\n",
    "with open(\"../../data/raw_data/annotations/mm10_venn.pkl\", \"rb\") as f:\n",
    "    subsets = pickle.load(f)\n",
    "venn3(subsets=subsets,\n",
    "      set_labels=('PolyAsites2.0', 'PolyA_DBV3.2', 'Gencode'))\n",
    "plt.show()\n",
    "plt.savefig(\"../../data/raw_data/annotations/venn3.svg\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter 3' most exon and single exon\n",
    "gencode_exon_df_filtered = gencode_exon_df[gencode_exon_df['exon_type'].isin(['3\\' most exon', 'Single exon'])]\n",
    "downstream_len=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bisect\n",
    "def find_next_start(current, sorted_list, strand):\n",
    "    # 使用二分查找找到比current_stop大的最小的start\n",
    "    if strand == '+':\n",
    "        idx = bisect.bisect_right(sorted_list, current)\n",
    "    elif strand == '-':\n",
    "        idx = bisect.bisect_right(sorted_list, current) - 1\n",
    "    else:\n",
    "        raise ValueError('Strand must be + or -')\n",
    "\n",
    "    if idx != len(sorted_list) and idx != -1:\n",
    "        return sorted_list[idx]\n",
    "    else:\n",
    "        print(f\"current: {current}, idx: {idx}, strand: {strand},sorted_list: {sorted_list}\")\n",
    "        raise ValueError('No next start found')\n",
    "\n",
    "chrs = gencode_genebody_df['chr'].unique()\n",
    "gencode_downstream_df = pd.DataFrame()\n",
    "for chr in chrs:\n",
    "    chr_genbody_df = gencode_genebody_df[gencode_genebody_df['chr'] == chr]\n",
    "    chr_exon_df = gencode_exon_df_filtered[gencode_exon_df_filtered['chr'] == chr]\n",
    "    #chr_df = pd.merge(chr_genbody_df, chr_exon_df[['chr', 'start', 'stop', 'gene_id', 'next_start']], left_on='gene_id', right_on='gene_id', how='left')\n",
    "    \n",
    "    chr_genbody_df_pos = chr_genbody_df[chr_genbody_df['strand'] == '+'].sort_values(['start'])\n",
    "    chr_genbody_df_neg = chr_genbody_df[chr_genbody_df['strand'] == '-'].sort_values(['stop'], ascending=False)\n",
    "    chr_genbody_df_pos['next_start'] = chr_genbody_df_pos['stop'] + downstream_len\n",
    "    chr_genbody_df_neg['next_start'] = chr_genbody_df_neg['start'] - downstream_len\n",
    "    chr_genbody_df_pos.loc[chr_genbody_df_pos.index[:-1], 'next_start'] = chr_genbody_df_pos['start'].shift(-1)[:-1]\n",
    "    chr_genbody_df_neg.loc[chr_genbody_df_neg.index[:-1], 'next_start'] = chr_genbody_df_neg['stop'].shift(-1)[:-1]\n",
    "   \n",
    "    exceptional_gene_pos = chr_genbody_df_pos[chr_genbody_df_pos['next_start'] < chr_genbody_df_pos['stop']]\n",
    "    exceptional_gene_neg = chr_genbody_df_neg[chr_genbody_df_neg['next_start'] > chr_genbody_df_neg['start']]\n",
    "\n",
    "\n",
    "    sorted_starts = sorted(chr_genbody_df_pos['start'].unique())\n",
    "    sorted_stops = sorted(chr_genbody_df_neg['stop'].unique())\n",
    "\n",
    "    if len(exceptional_gene_pos) > 0:\n",
    "        for index, gene in exceptional_gene_pos.iterrows():\n",
    "            try:\n",
    "                chr_genbody_df_pos.loc[index, 'next_start'] = find_next_start(gene['stop'], sorted_starts, '+')\n",
    "            except ValueError:\n",
    "                chr_genbody_df_pos.loc[index, 'next_start'] = gene['stop'] + downstream_len\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "    if len(exceptional_gene_neg) > 0:\n",
    "        for index, gene in exceptional_gene_neg.iterrows():\n",
    "            try:\n",
    "                chr_genbody_df_neg.loc[index, 'next_start'] = find_next_start(gene['start'], sorted_stops, '-')\n",
    "            except ValueError:\n",
    "                chr_genbody_df_neg.loc[index, 'next_start'] = gene['start'] - downstream_len\n",
    "            except Exception as e:\n",
    "                raise e\n",
    "    chr_exon_df_pos = chr_exon_df[chr_exon_df['strand'] == '+'].sort_values(['start'])\n",
    "    chr_exon_df_neg = chr_exon_df[chr_exon_df['strand'] == '-'].sort_values(['stop'], ascending=False)\n",
    "\n",
    "    chr_exon_df_pos = pd.merge(chr_exon_df_pos, chr_genbody_df_pos[[\"gene_id\",\"next_start\"]],  left_on='gene_id', right_on='gene_id', how='left')\n",
    "    chr_exon_df_neg = pd.merge(chr_exon_df_neg, chr_genbody_df_neg[[\"gene_id\",\"next_start\"]],  left_on='gene_id', right_on='gene_id', how='left')\n",
    "\n",
    "    chr_exon_df_pos['downstream_start'] = chr_exon_df_pos[\"stop\"]\n",
    "    chr_exon_df_pos['downstream_stop'] = np.where(chr_exon_df_pos['downstream_start'] + downstream_len > chr_exon_df_pos['next_start'],\n",
    "                                        chr_exon_df_pos['next_start'], chr_exon_df_pos['downstream_start'] + downstream_len)\n",
    "    chr_exon_df_neg['downstream_stop'] = chr_exon_df_neg[\"start\"]\n",
    "    chr_exon_df_neg['downstream_start'] = np.where(chr_exon_df_neg['downstream_stop'] - downstream_len < chr_exon_df_neg['next_start'],\n",
    "                                        chr_exon_df_neg['next_start'], chr_exon_df_neg['downstream_stop'] - downstream_len)\n",
    "    \n",
    "    chr_downstream_df = pd.concat([chr_exon_df_neg, chr_exon_df_pos])[['chr', 'downstream_start', 'downstream_stop', 'gene_id', 'score', 'strand', 'gene_name']]\n",
    "    chr_downstream_df[\"exon_type\"] = \"downstream\"\n",
    "\n",
    "    gencode_downstream_df = pd.concat([gencode_downstream_df, chr_downstream_df])\n",
    "gencode_downstream_df = gencode_downstream_df.rename(columns={\"downstream_start\": \"start\", \"downstream_stop\": \"stop\"})\n",
    "gencode_downstream_df = gencode_downstream_df.drop_duplicates(['chr', 'start', 'stop', 'gene_id', 'score', 'strand', 'gene_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "gencode_genebody_df_final = pd.concat([gencode_genebody_df, gencode_downstream_df])\n",
    "gencode_exon_df_final = pd.concat([gencode_exon_df, gencode_downstream_df])\n",
    "gencode_exon_df_final = gencode_exon_df_final.drop_duplicates([\"chr\", \"start\", \"stop\", \"gene_id\", \"exon_type\", \"strand\"], keep=\"first\")\n",
    "\n",
    "gencode_exon_df_final[\"name\"] = gencode_exon_df_final[\"gene_id\"].str.cat(gencode_exon_df_final[[\"gene_name\", \"exon_type\"]], sep=\":\")\n",
    "gencode_genebody_df_final[\"name\"] = gencode_genebody_df_final[\"gene_id\"].str.cat(gencode_genebody_df_final[[\"gene_name\"]], sep=\":\")\n",
    "\n",
    "gencode_exon_bed = BedTool.from_dataframe(gencode_exon_df_final[[\"chr\", \"start\", \"stop\", \"name\", \"score\", \"strand\"]])\n",
    "gencode_genebody_bed = BedTool.from_dataframe(gencode_genebody_df_final[[\"chr\", \"start\", \"stop\", \"name\", \"score\", \"strand\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated_pas = merged_polyasites_addall.to_dataframe()\n",
    "integrated_pas.columns = [\"chr\", \"start\", \"stop\", \"name\", \"score\", \"strand\"]\n",
    "integrated_pas[\"name\"] = integrated_pas[\"chr\"].str.cat(integrated_pas[[\"start\", \"stop\", \"strand\",]].astype(str), sep=\":\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated_pas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated_pas_bed = BedTool.from_dataframe(integrated_pas)\n",
    "intersected_pas_genebody = integrated_pas_bed.intersect(gencode_genebody_bed, wa=True, wb=True, s=True)\n",
    "intersected_pas_exon = integrated_pas_bed.intersect(gencode_exon_bed, wa=True, wb=True, s=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersected_pas_genebody_df = intersected_pas_genebody.to_dataframe()\n",
    "intersected_pas_exon_df = intersected_pas_exon.to_dataframe()\n",
    "\n",
    "intersected_pas_genebody_df[['gene_id', 'gene_name']] = intersected_pas_genebody_df.iloc[:,9].str.split(':', expand=True)\n",
    "intersected_pas_exon_df[['gene_id', 'gene_name', 'exon_type']] = intersected_pas_exon_df.iloc[:,9].str.split(':', expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "intronic_pas = intersected_pas_genebody_df[~intersected_pas_genebody_df[\"name\"].isin(intersected_pas_exon_df[\"name\"])][\"name\"].copy().drop_duplicates().tolist()\n",
    "intersected_pas_intron_df = intersected_pas_genebody_df[intersected_pas_genebody_df[\"name\"].isin(intronic_pas)].copy()\n",
    "intersected_pas_intron_df[\"exon_type\"] = \"intron\"\n",
    "intersected_pas_df = pd.concat([intersected_pas_exon_df, intersected_pas_intron_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "priority_dict = {\n",
    "    \"3' most exon\": 5,\n",
    "    \"Single exon\": 4,\n",
    "    \"Internal exon\": 3,\n",
    "    \"5' most exon\": 2,\n",
    "    \"downstream\": 0,\n",
    "    \"intron\": 1\n",
    "}\n",
    "intersected_pas_df['priority'] = intersected_pas_df['exon_type'].map(priority_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "intersected_pas_df_max_priority = intersected_pas_df.groupby('name')['priority'].max()\n",
    "intersected_pas_df_with_max_priority = intersected_pas_df.merge(intersected_pas_df_max_priority, on='name', suffixes=('', '_max'))\n",
    "intersected_pas_df_multiple_max = intersected_pas_df_with_max_priority[intersected_pas_df_with_max_priority['priority'] == intersected_pas_df_with_max_priority['priority_max']]\n",
    "grouped_df = intersected_pas_df_multiple_max.drop_duplicates([\"name\", \"gene_id\"]).groupby(\"name\").size()\n",
    "unknown_pas = grouped_df[grouped_df > 1].index.tolist()\n",
    "intersected_pas_df_drop_duplicates = intersected_pas_df.sample(frac=1).sort_values(['name', 'priority'], ascending=[True, False]).groupby('name').first().reset_index()\n",
    "intersected_pas_df_drop_duplicates.loc[intersected_pas_df_drop_duplicates[\"name\"].isin(unknown_pas), [\"gene_id\", \"gene_name\"]] = \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "intronic_pas = intersected_pas_df_drop_duplicates[intersected_pas_df_drop_duplicates[\"exon_type\"] == \"intron\"][\"name\"].copy().drop_duplicates().tolist()\n",
    "most3Exon = intersected_pas_df_drop_duplicates[intersected_pas_df_drop_duplicates[\"exon_type\"] == \"3' most exon\"][\"name\"].copy().drop_duplicates().tolist()\n",
    "most5Exon = intersected_pas_df_drop_duplicates[intersected_pas_df_drop_duplicates[\"exon_type\"] == \"5' most exon\"][\"name\"].copy().drop_duplicates().tolist()\n",
    "singleExon = intersected_pas_df_drop_duplicates[intersected_pas_df_drop_duplicates[\"exon_type\"] == \"Single exon\"][\"name\"].copy().drop_duplicates().tolist()\n",
    "internalExon = intersected_pas_df_drop_duplicates[intersected_pas_df_drop_duplicates[\"exon_type\"] == \"Internal exon\"][\"name\"].copy().drop_duplicates().tolist()\n",
    "downstream = intersected_pas_df_drop_duplicates[intersected_pas_df_drop_duplicates[\"exon_type\"] == \"downstream\"][\"name\"].copy().drop_duplicates().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated_pas[\"pas_type\"] = \"intergenic\"\n",
    "integrated_pas.loc[integrated_pas[\"name\"].isin(intronic_pas), \"pas_type\"] = \"Intron\"\n",
    "integrated_pas.loc[integrated_pas[\"name\"].isin(most5Exon), \"pas_type\"] = \"5' most exon\"\n",
    "integrated_pas.loc[integrated_pas[\"name\"].isin(most3Exon), \"pas_type\"] = \"3' most exon\"\n",
    "integrated_pas.loc[integrated_pas[\"name\"].isin(downstream), \"pas_type\"] = \"downstream\"\n",
    "integrated_pas.loc[integrated_pas[\"name\"].isin(singleExon), \"pas_type\"] = \"Single exon\"\n",
    "integrated_pas.loc[integrated_pas[\"name\"].isin(internalExon), \"pas_type\"] = \"Internal exon\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated_pas_final = pd.merge(left=integrated_pas, right=intersected_pas_df_drop_duplicates, on=\"name\", how=\"left\", suffixes=('', '_y'), ).fillna(\"unknown\").loc[:, [\"chr\", \"start\", \"stop\", \"name\", \"score\", \"strand\", \"pas_type\", \"gene_id\", \"gene_name\"]]\n",
    "integrated_pas_final_bed_df = integrated_pas_final.copy()\n",
    "integrated_pas_final_bed = BedTool.from_dataframe(integrated_pas_final_bed_df)\n",
    "# add polyadb exon information for pas\n",
    "# intersect_polyadb = get_overlapping_pases(integrated_pas_final_bed, polyadb_bed_merged).to_dataframe(header=None)\n",
    "intersect_polyadb = integrated_pas_final_bed.window(polyadb_bed_merged, w=20, sm=True).to_dataframe(header=None)\n",
    "intersect_polyadb[[\"polyadb_type\",\"polyadb_gene\"]] = intersect_polyadb.iloc[:,12].str.split(':', expand=True)\n",
    "intersect_polyadb = intersect_polyadb[~intersect_polyadb[\"polyadb_gene\"].isin([\"unknown\", \"na\", \"nan\"])]\n",
    "intersect_polyadb[\"polyadb_type\"] = intersect_polyadb[\"polyadb_type\"].str.replace(\"_\", \" \")\n",
    "intersect_polyadb = intersect_polyadb.rename(columns={3: \"name\", 6: \"pas_type\"})\n",
    "intersect_polyadb = intersect_polyadb[[\"name\", \"polyadb_type\"]]\n",
    "# add polyasites TE information for pas\n",
    "# intersect_polyasites = get_overlapping_pases(integrated_pas_final_bed, polyasites_bed_merged).to_dataframe(header=None)\n",
    "intersect_polyasites = integrated_pas_final_bed.window(merged_polyasites, w=20, sm=True).to_dataframe(header=None)\n",
    "intersect_polyasites = intersect_polyasites.rename(columns={3: \"name\", 12:\"polyasites_type\"})\n",
    "intersect_polyasites = intersect_polyasites[intersect_polyasites[\"polyasites_type\"] == \"TE\"][[\"name\", \"polyasites_type\"]]\n",
    "intersect_polyasites[\"polyasites_type\"] = \"3' most exon\"\n",
    "\n",
    "integrated_pas_final_add_annotation = pd.merge(integrated_pas_final, intersect_polyadb, how=\"left\")\n",
    "integrated_pas_final_add_annotation = pd.merge(integrated_pas_final_add_annotation, intersect_polyasites, how=\"left\")\n",
    "\n",
    "integrated_pas_final_add_annotation[\"polyadb_type\"].combine_first(integrated_pas_final_add_annotation[\"polyasites_type\"]).combine_first(integrated_pas_final_add_annotation[\"pas_type\"])\n",
    "integrated_pas_final_add_annotation[\"integrated_pas_type\"] = integrated_pas_final_add_annotation[\"polyadb_type\"].combine_first(integrated_pas_final_add_annotation[\"polyasites_type\"]).combine_first(integrated_pas_final_add_annotation[\"pas_type\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated_pas_final = integrated_pas_final_add_annotation.loc[:, [\"chr\", \"start\", \"stop\", \"name\", \"score\", \"strand\", \"integrated_pas_type\", \"gene_id\", \"gene_name\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integrated_pas_final.to_csv(\"mouse_integrated_pas.bed\", sep=\"\\t\", index=False, header=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apasim",
   "language": "python",
   "name": "apasim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
